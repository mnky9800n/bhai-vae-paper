{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BHAI VAE: Semi-Supervised Variational Autoencoder for Ocean Drilling Data\n",
    "\n",
    "This notebook implements the complete analysis pipeline for the BHAI VAE paper:\n",
    "\n",
    "1. **Data Pipeline**: Download from Zenodo, process to 20cm bins\n",
    "2. **Descriptive Figures**: Expedition map, dataset overview, lithology counts, variable distributions\n",
    "3. **Model Training**: Unsupervised and Semi-supervised VAE with masked encoding\n",
    "4. **Model Evaluation**: SVM classification, clustering metrics (ARI), reconstruction quality\n",
    "5. **Evaluation Figures**: Reconstruction scatter, ROC comparison, UMAP visualization\n",
    "6. **Network Diagrams**: Architecture visualization from PyTorch specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (r2_score, roc_curve, auc, accuracy_score, \n",
    "                             precision_recall_fscore_support, adjusted_rand_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# UMAP\n",
    "import umap\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Download\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Mapping\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    HAS_CARTOPY = True\n",
    "except ImportError:\n",
    "    HAS_CARTOPY = False\n",
    "    print(\"Warning: cartopy not available, expedition map will be skipped\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add models directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from models.vae import VAE, SemiSupervisedVAE, DistributionAwareScaler, FEATURE_COLS\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "FIGURES_DIR = BASE_DIR / \"figures\"\n",
    "\n",
    "# Create directories\n",
    "for d in [RAW_DATA_DIR, PROCESSED_DATA_DIR, MODEL_DIR, FIGURES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data source\n",
    "ZENODO_DOI = \"10.5281/zenodo.7484524\"\n",
    "ZENODO_RECORD_ID = \"7484524\"\n",
    "\n",
    "# Model architecture\n",
    "INPUT_DIM = 6\n",
    "LATENT_DIM = 10\n",
    "HIDDEN_DIMS = [64, 32]\n",
    "\n",
    "# Training hyperparameters\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3\n",
    "MASK_RATIO = 0.10  # 10% masked encoding\n",
    "\n",
    "# Annealing schedules\n",
    "BETA_START = 1e-10\n",
    "BETA_END = 0.075\n",
    "BETA_ANNEAL_EPOCHS = 50\n",
    "\n",
    "ALPHA_START = 0.0\n",
    "ALPHA_END = 0.01\n",
    "ALPHA_ANNEAL_EPOCHS = 50\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    \"Bulk density (GRA)\",\n",
    "    \"Magnetic susceptibility (instr. units)\",\n",
    "    \"NGR total counts (cps)\",\n",
    "    \"R\", \"G\", \"B\"\n",
    "]\n",
    "\n",
    "FEATURE_NAMES_SHORT = ['Bulk Density', 'Mag. Susc.', 'NGR', 'R', 'G', 'B']\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Base dir: {BASE_DIR}\")\n",
    "print(f\"  Epochs: {N_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Mask ratio: {MASK_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"Random seed set to {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Download from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_zenodo(record_id, output_dir, files_to_download=None):\n",
    "    \"\"\"\n",
    "    Download files from Zenodo record.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    record_id : str\n",
    "        Zenodo record ID (e.g., \"7484524\")\n",
    "    output_dir : Path\n",
    "        Directory to save downloaded files\n",
    "    files_to_download : list, optional\n",
    "        Specific files to download. If None, downloads all files.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get record metadata\n",
    "    api_url = f\"https://zenodo.org/api/records/{record_id}\"\n",
    "    response = requests.get(api_url)\n",
    "    response.raise_for_status()\n",
    "    record = response.json()\n",
    "    \n",
    "    print(f\"Record: {record['metadata']['title']}\")\n",
    "    print(f\"DOI: {record['doi']}\")\n",
    "    print(f\"Files available: {len(record['files'])}\")\n",
    "    \n",
    "    downloaded = []\n",
    "    for file_info in record['files']:\n",
    "        filename = file_info['key']\n",
    "        \n",
    "        # Filter files if specified\n",
    "        if files_to_download and filename not in files_to_download:\n",
    "            continue\n",
    "            \n",
    "        output_path = output_dir / filename\n",
    "        \n",
    "        # Skip if already exists\n",
    "        if output_path.exists():\n",
    "            print(f\"  {filename} already exists, skipping\")\n",
    "            downloaded.append(output_path)\n",
    "            continue\n",
    "        \n",
    "        # Download\n",
    "        print(f\"  Downloading {filename} ({file_info['size'] / 1e6:.1f} MB)...\")\n",
    "        download_url = file_info['links']['self']\n",
    "        \n",
    "        with requests.get(download_url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(output_path, 'wb') as f:\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True, desc=filename) as pbar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "        \n",
    "        downloaded.append(output_path)\n",
    "        print(f\"    Saved to {output_path}\")\n",
    "    \n",
    "    return downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files we need from Zenodo\n",
    "ZENODO_FILES = [\n",
    "    \"GRA_DataLITH.csv\",\n",
    "    \"MS_DataLITH.csv\",\n",
    "    \"NGR_DataLITH.csv\",\n",
    "    \"RSC_DataLITH.csv\"  # Contains L*a*b* and RGB\n",
    "]\n",
    "\n",
    "# Check if raw data exists, download if not\n",
    "raw_files_exist = all((RAW_DATA_DIR / f).exists() for f in ZENODO_FILES)\n",
    "\n",
    "if not raw_files_exist:\n",
    "    print(\"Downloading raw data from Zenodo...\")\n",
    "    downloaded = download_from_zenodo(ZENODO_RECORD_ID, RAW_DATA_DIR, ZENODO_FILES)\n",
    "    print(f\"\\nDownloaded {len(downloaded)} files\")\n",
    "else:\n",
    "    print(\"Raw data already exists in\", RAW_DATA_DIR)\n",
    "    for f in ZENODO_FILES:\n",
    "        path = RAW_DATA_DIR / f\n",
    "        if path.exists():\n",
    "            print(f\"  {f}: {path.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_data(raw_dir, output_path, bin_size_cm=20):\n",
    "    \"\"\"\n",
    "    Process raw LILY data files into training format.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load each measurement type\n",
    "    2. Create depth bins (default 20cm)\n",
    "    3. Aggregate measurements within bins (mean)\n",
    "    4. Inner join all measurements\n",
    "    5. Filter for complete samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_dir : Path\n",
    "        Directory containing raw CSV files\n",
    "    output_path : Path\n",
    "        Output path for processed CSV\n",
    "    bin_size_cm : int\n",
    "        Depth bin size in centimeters\n",
    "    \"\"\"\n",
    "    raw_dir = Path(raw_dir)\n",
    "    bin_size_m = bin_size_cm / 100.0\n",
    "    \n",
    "    print(f\"Processing raw data with {bin_size_cm}cm bins...\")\n",
    "    \n",
    "    # Load GRA (Bulk Density)\n",
    "    print(\"  Loading GRA...\")\n",
    "    gra = pd.read_csv(raw_dir / \"GRA_DataLITH.csv\")\n",
    "    gra['Depth_Bin'] = (gra['Top depth CSF-A (m)'] // bin_size_m) * bin_size_m\n",
    "    gra_agg = gra.groupby(['Borehole_ID', 'Depth_Bin']).agg({\n",
    "        'Bulk density (GRA)': 'mean',\n",
    "        'Principal': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Load MS (Magnetic Susceptibility)\n",
    "    print(\"  Loading MS...\")\n",
    "    ms = pd.read_csv(raw_dir / \"MS_DataLITH.csv\")\n",
    "    ms['Depth_Bin'] = (ms['Top depth CSF-A (m)'] // bin_size_m) * bin_size_m\n",
    "    ms_agg = ms.groupby(['Borehole_ID', 'Depth_Bin']).agg({\n",
    "        'Magnetic susceptibility (instr. units)': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Load NGR\n",
    "    print(\"  Loading NGR...\")\n",
    "    ngr = pd.read_csv(raw_dir / \"NGR_DataLITH.csv\")\n",
    "    ngr['Depth_Bin'] = (ngr['Top depth CSF-A (m)'] // bin_size_m) * bin_size_m\n",
    "    ngr_agg = ngr.groupby(['Borehole_ID', 'Depth_Bin']).agg({\n",
    "        'NGR total counts (cps)': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Load RSC (RGB/L*a*b*)\n",
    "    print(\"  Loading RSC...\")\n",
    "    rsc = pd.read_csv(raw_dir / \"RSC_DataLITH.csv\")\n",
    "    rsc['Depth_Bin'] = (rsc['Top depth CSF-A (m)'] // bin_size_m) * bin_size_m\n",
    "    rsc_agg = rsc.groupby(['Borehole_ID', 'Depth_Bin']).agg({\n",
    "        'R': 'mean', 'G': 'mean', 'B': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge all datasets (inner join for complete samples)\n",
    "    print(\"  Merging datasets...\")\n",
    "    merged = gra_agg.merge(ms_agg, on=['Borehole_ID', 'Depth_Bin'], how='inner')\n",
    "    merged = merged.merge(ngr_agg, on=['Borehole_ID', 'Depth_Bin'], how='inner')\n",
    "    merged = merged.merge(rsc_agg, on=['Borehole_ID', 'Depth_Bin'], how='inner')\n",
    "    \n",
    "    # Remove rows with any NaN in features\n",
    "    feature_cols = ['Bulk density (GRA)', 'Magnetic susceptibility (instr. units)',\n",
    "                   'NGR total counts (cps)', 'R', 'G', 'B']\n",
    "    before = len(merged)\n",
    "    merged = merged.dropna(subset=feature_cols + ['Principal'])\n",
    "    after = len(merged)\n",
    "    \n",
    "    print(f\"  Samples: {before} -> {after} (dropped {before - after} incomplete)\")\n",
    "    \n",
    "    # Save\n",
    "    merged.to_csv(output_path, index=False)\n",
    "    print(f\"  Saved to {output_path}\")\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing processed data or process from raw\n",
    "PROCESSED_DATA_PATH = DATA_DIR / \"vae_training_data_v2_20cm.csv\"\n",
    "\n",
    "if PROCESSED_DATA_PATH.exists():\n",
    "    print(f\"Loading existing processed data from {PROCESSED_DATA_PATH}\")\n",
    "    df = pd.read_csv(PROCESSED_DATA_PATH)\n",
    "else:\n",
    "    print(\"Processing raw data...\")\n",
    "    df = process_raw_data(RAW_DATA_DIR, PROCESSED_DATA_PATH, bin_size_cm=20)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nSample counts by lithology (top 10):\")\n",
    "print(df['Principal'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "print(\"Preparing data for training...\")\n",
    "\n",
    "# Extract features\n",
    "X_raw = df[FEATURE_COLUMNS].values\n",
    "print(f\"Features shape: {X_raw.shape}\")\n",
    "\n",
    "# Scale using DistributionAwareScaler\n",
    "scaler = DistributionAwareScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "print(f\"Scaled features shape: {X_scaled.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Principal'].values)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of lithology classes: {n_classes}\")\n",
    "\n",
    "# Extract expedition info\n",
    "df['Expedition'] = df['Borehole_ID'].str.extract(r'^(\\d+)')[0].astype(int)\n",
    "expeditions = df['Expedition'].unique()\n",
    "print(f\"Number of expeditions: {len(expeditions)}\")\n",
    "print(f\"Expeditions: {sorted(expeditions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Descriptive Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Expedition Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IODP expedition coordinates (approximate central locations)\n",
    "# Based on IODP expedition reports\n",
    "EXPEDITION_COORDS = {\n",
    "    346: (40.5, 138.5),   # Asian Monsoon (Sea of Japan)\n",
    "    349: (15.5, 117.0),   # South China Sea\n",
    "    350: (32.0, 139.5),   # Izu-Bonin-Mariana\n",
    "    351: (30.5, 134.5),   # Izu-Bonin-Mariana\n",
    "    352: (28.5, 141.0),   # Izu-Bonin-Mariana\n",
    "    353: (8.0, 87.0),     # iMonsoon\n",
    "    354: (8.0, 88.5),     # Bengal Fan\n",
    "    355: (22.0, 68.0),    # Arabian Sea Monsoon\n",
    "    356: (-28.0, 113.0),  # Indonesian Throughflow\n",
    "    359: (5.0, 73.0),     # Maldives Monsoon and Sea Level\n",
    "    360: (-42.0, 9.0),    # SW Indian Ridge\n",
    "    361: (-41.5, -8.0),   # S African Climates\n",
    "    362: (-2.0, 102.0),   # Sumatra Seismogenesis\n",
    "    363: (-5.0, 142.0),   # W Pacific Warm Pool\n",
    "    366: (13.5, 147.0),   # Mariana Convergent Margin\n",
    "    367: (18.5, 116.5),   # South China Sea Rifted Margin\n",
    "    368: (19.0, 116.0),   # South China Sea Rifted Margin\n",
    "    \"368X\": (19.5, 115.5), # South China Sea Rifted Margin Extension\n",
    "    369: (-33.5, 104.0),  # Australia Cretaceous Climate\n",
    "    371: (-34.0, 163.5),  # Tasman Frontier\n",
    "    372: (-35.0, 174.0),  # Creeping Gas Hydrate Slides\n",
    "    374: (-77.0, 175.0),  # Ross Sea West Antarctic Ice Sheet\n",
    "    375: (-39.0, 178.5),  # Hikurangi Subduction Margin\n",
    "    376: (-37.5, 177.0),  # Brothers Arc Flux\n",
    "    379: (-57.5, -92.5),  # Amundsen Sea West Antarctic Ice Sheet\n",
    "}\n",
    "\n",
    "if HAS_CARTOPY:\n",
    "    # Count samples per expedition\n",
    "    exp_counts = df.groupby('Expedition').size().to_dict()\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson(central_longitude=150))\n",
    "    \n",
    "    # Add map features\n",
    "    ax.set_global()\n",
    "    ax.add_feature(cfeature.LAND, facecolor='#f5f5dc', edgecolor='gray', linewidth=0.5)\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='#add8e6')\n",
    "    ax.gridlines(draw_labels=False, linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Plot expeditions\n",
    "    for exp, (lat, lon) in EXPEDITION_COORDS.items():\n",
    "        exp_int = int(exp) if isinstance(exp, (int, float)) else exp\n",
    "        count = exp_counts.get(exp_int, exp_counts.get(str(exp), 0))\n",
    "        \n",
    "        # Size based on sample count\n",
    "        size = np.sqrt(count) * 2 + 50\n",
    "        \n",
    "        ax.scatter(lon, lat, s=size, c='#cd5c5c', alpha=0.7,\n",
    "                  transform=ccrs.PlateCarree(), zorder=5, edgecolors='darkred', linewidth=0.5)\n",
    "        \n",
    "        # Label\n",
    "        ax.text(lon + 3, lat + 2, str(exp), fontsize=8,\n",
    "               transform=ccrs.PlateCarree(), zorder=6)\n",
    "    \n",
    "    # Legend\n",
    "    legend_sizes = [1000, 10000, 40000]\n",
    "    legend_labels = ['1,000', '10,000', '40,000']\n",
    "    legend_handles = [plt.scatter([], [], s=np.sqrt(s)*2+50, c='#cd5c5c', \n",
    "                                  edgecolors='darkred', linewidth=0.5, alpha=0.7)\n",
    "                     for s in legend_sizes]\n",
    "    \n",
    "    legend = ax.legend(legend_handles, legend_labels, \n",
    "                      title='Samples', loc='upper right',\n",
    "                      frameon=True, fontsize=9)\n",
    "    \n",
    "    ax.set_title('IODP Boreholes from the LILY Dataset', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'fig_lily_expedition_map.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {FIGURES_DIR / 'fig_lily_expedition_map.png'}\")\n",
    "else:\n",
    "    print(\"Skipping expedition map (cartopy not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Panel 1: Samples per expedition\n",
    "ax = axes[0, 0]\n",
    "exp_counts = df['Expedition'].value_counts().sort_index()\n",
    "bars = ax.bar(exp_counts.index.astype(str), exp_counts.values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Expedition', fontsize=11)\n",
    "ax.set_ylabel('Number of Samples', fontsize=11)\n",
    "ax.set_title('Samples per Expedition', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 2: Boreholes per expedition\n",
    "ax = axes[0, 1]\n",
    "borehole_counts = df.groupby('Expedition')['Borehole_ID'].nunique().sort_index()\n",
    "ax.bar(borehole_counts.index.astype(str), borehole_counts.values, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Expedition', fontsize=11)\n",
    "ax.set_ylabel('Number of Boreholes', fontsize=11)\n",
    "ax.set_title('Boreholes per Expedition', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 3: Depth distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(df['Depth_Bin'], bins=100, color='seagreen', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Depth (m)', fontsize=11)\n",
    "ax.set_ylabel('Frequency', fontsize=11)\n",
    "ax.set_title('Depth Distribution', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel 4: Summary statistics text\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "Dataset Summary\n",
    "{'='*40}\n",
    "\n",
    "Total samples: {len(df):,}\n",
    "Total boreholes: {df['Borehole_ID'].nunique():,}\n",
    "Total expeditions: {df['Expedition'].nunique()}\n",
    "Lithology classes: {n_classes}\n",
    "\n",
    "Depth range: {df['Depth_Bin'].min():.1f} - {df['Depth_Bin'].max():.1f} m\n",
    "Mean depth: {df['Depth_Bin'].mean():.1f} m\n",
    "\n",
    "Features:\n",
    "  • Bulk density (GRA)\n",
    "  • Magnetic susceptibility\n",
    "  • Natural gamma radiation (NGR)\n",
    "  • RGB color (R, G, B)\n",
    "\n",
    "Bin size: 20 cm\n",
    "\"\"\"\n",
    "ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_lily_dataset.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_lily_dataset.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Lithology Class Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Get top 30 lithology classes\n",
    "lith_counts = df['Principal'].value_counts().head(30)\n",
    "\n",
    "# Color by frequency tier\n",
    "colors = []\n",
    "for count in lith_counts.values:\n",
    "    if count > 10000:\n",
    "        colors.append('#1f77b4')  # Blue for most common\n",
    "    elif count > 5000:\n",
    "        colors.append('#2ca02c')  # Green\n",
    "    elif count > 1000:\n",
    "        colors.append('#ff7f0e')  # Orange\n",
    "    else:\n",
    "        colors.append('#d62728')  # Red for rare\n",
    "\n",
    "bars = ax.barh(range(len(lith_counts)), lith_counts.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax.set_yticks(range(len(lith_counts)))\n",
    "ax.set_yticklabels(lith_counts.index, fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Number of Samples', fontsize=12)\n",
    "ax.set_title('Top 30 Lithology Classes by Sample Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, (count, bar) in enumerate(zip(lith_counts.values, bars)):\n",
    "    ax.text(count + 200, i, f'{count:,}', va='center', fontsize=8)\n",
    "\n",
    "# Legend for colors\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#1f77b4', edgecolor='black', label='>10,000'),\n",
    "    Patch(facecolor='#2ca02c', edgecolor='black', label='5,000-10,000'),\n",
    "    Patch(facecolor='#ff7f0e', edgecolor='black', label='1,000-5,000'),\n",
    "    Patch(facecolor='#d62728', edgecolor='black', label='<1,000'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, title='Sample Count', loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_lily_lithology_counts.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_lily_lithology_counts.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Variable Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "for i, (col, name, color) in enumerate(zip(FEATURE_COLUMNS, FEATURE_NAMES_SHORT, colors)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    data = df[col].dropna()\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(data, bins=100, color=color, edgecolor='black', alpha=0.7, density=True)\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = f'μ={data.mean():.2f}\\nσ={data.std():.2f}\\nn={len(data):,}'\n",
    "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel(name, fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(f'{name} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_lily_variables_dist.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_lily_variables_dist.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definitions\n",
    "\n",
    "Models are imported from `models/vae.py`. Here we verify the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model architectures\n",
    "print(\"Unsupervised VAE Architecture:\")\n",
    "print(\"=\"*50)\n",
    "model_unsup_test = VAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM, hidden_dims=HIDDEN_DIMS)\n",
    "print(model_unsup_test)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_unsup_test.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Semi-Supervised VAE Architecture:\")\n",
    "print(\"=\"*50)\n",
    "model_semisup_test = SemiSupervisedVAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM, \n",
    "                                        hidden_dims=HIDDEN_DIMS, n_classes=n_classes)\n",
    "print(model_semisup_test)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model_semisup_test.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(epoch, beta_start=BETA_START, beta_end=BETA_END, anneal_epochs=BETA_ANNEAL_EPOCHS):\n",
    "    \"\"\"Get beta value for current epoch (linear annealing).\"\"\"\n",
    "    if epoch >= anneal_epochs:\n",
    "        return beta_end\n",
    "    return beta_start + (beta_end - beta_start) * (epoch / anneal_epochs)\n",
    "\n",
    "def get_alpha(epoch, alpha_start=ALPHA_START, alpha_end=ALPHA_END, anneal_epochs=ALPHA_ANNEAL_EPOCHS):\n",
    "    \"\"\"Get alpha value for current epoch (linear annealing).\"\"\"\n",
    "    if epoch >= anneal_epochs:\n",
    "        return alpha_end\n",
    "    return alpha_start + (alpha_end - alpha_start) * (epoch / anneal_epochs)\n",
    "\n",
    "def apply_mask(x, mask_ratio=MASK_RATIO):\n",
    "    \"\"\"\n",
    "    Apply random masking to input tensor.\n",
    "    \n",
    "    Randomly sets mask_ratio fraction of values to 0.\n",
    "    Returns masked tensor and mask.\n",
    "    \"\"\"\n",
    "    mask = torch.rand_like(x) < mask_ratio\n",
    "    x_masked = x.clone()\n",
    "    x_masked[mask] = 0\n",
    "    return x_masked, mask\n",
    "\n",
    "# Visualize annealing schedules\n",
    "epochs = np.arange(N_EPOCHS)\n",
    "betas = [get_beta(e) for e in epochs]\n",
    "alphas = [get_alpha(e) for e in epochs]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(epochs, betas, 'b-', linewidth=2)\n",
    "ax.axhline(y=BETA_END, color='r', linestyle='--', alpha=0.5, label=f'β_end = {BETA_END}')\n",
    "ax.axvline(x=BETA_ANNEAL_EPOCHS, color='g', linestyle='--', alpha=0.5, label=f'Anneal end = {BETA_ANNEAL_EPOCHS}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('β (KL weight)')\n",
    "ax.set_title('β-Annealing Schedule')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(epochs, alphas, 'orange', linewidth=2)\n",
    "ax.axhline(y=ALPHA_END, color='r', linestyle='--', alpha=0.5, label=f'α_end = {ALPHA_END}')\n",
    "ax.axvline(x=ALPHA_ANNEAL_EPOCHS, color='g', linestyle='--', alpha=0.5, label=f'Anneal end = {ALPHA_ANNEAL_EPOCHS}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('α (Classification weight)')\n",
    "ax.set_title('α-Annealing Schedule (Semi-supervised)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Unsupervised VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unsupervised_vae(X, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE, \n",
    "                           mask_ratio=MASK_RATIO, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Train unsupervised VAE with masked encoding and β-annealing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Scaled input features\n",
    "    n_epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size\n",
    "    mask_ratio : float\n",
    "        Fraction of inputs to mask during training\n",
    "    device : torch.device\n",
    "        Device to train on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : VAE\n",
    "        Trained model\n",
    "    history : dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    set_seed(SEED)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(torch.FloatTensor(X))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
    "                           num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = VAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM, hidden_dims=HIDDEN_DIMS).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'loss': [], 'recon_loss': [], 'kl_loss': [], 'beta': []}\n",
    "    \n",
    "    # Training loop\n",
    "    pbar = tqdm(range(n_epochs), desc='Training Unsup VAE')\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_recon = 0\n",
    "        epoch_kl = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        beta = get_beta(epoch)\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            \n",
    "            # Apply masking\n",
    "            x_masked, mask = apply_mask(x_batch, mask_ratio)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x_masked)\n",
    "            \n",
    "            # Loss\n",
    "            loss, recon_loss, kl_loss = model.loss_function(recon, x_batch, mu, logvar, beta)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon += recon_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['recon_loss'].append(epoch_recon / n_batches)\n",
    "        history['kl_loss'].append(epoch_kl / n_batches)\n",
    "        history['beta'].append(beta)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{epoch_loss/n_batches:.1f}\",\n",
    "            'recon': f\"{epoch_recon/n_batches:.1f}\",\n",
    "            'kl': f\"{epoch_kl/n_batches:.1f}\",\n",
    "            'β': f\"{beta:.2e}\"\n",
    "        })\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train unsupervised VAE\n",
    "print(f\"Training unsupervised VAE on {len(X_scaled):,} samples...\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Epochs: {N_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Mask ratio: {MASK_RATIO}\")\n",
    "print(f\"  β-annealing: {BETA_START} → {BETA_END} over {BETA_ANNEAL_EPOCHS} epochs\")\n",
    "print()\n",
    "\n",
    "model_unsup, history_unsup = train_unsupervised_vae(X_scaled)\n",
    "\n",
    "# Save model\n",
    "torch.save(model_unsup.state_dict(), MODEL_DIR / 'model_unsup.pt')\n",
    "print(f\"\\nModel saved to {MODEL_DIR / 'model_unsup.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history_unsup['loss'], 'b-', linewidth=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Loss')\n",
    "ax.set_title('Unsupervised VAE: Total Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history_unsup['recon_loss'], 'g-', linewidth=1.5, label='Recon Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Reconstruction Loss')\n",
    "ax.set_title('Reconstruction Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(history_unsup['kl_loss'], 'r-', linewidth=1.5, label='KL Loss')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(history_unsup['beta'], 'b--', linewidth=1, alpha=0.7, label='β')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('KL Loss', color='r')\n",
    "ax2.set_ylabel('β', color='b')\n",
    "ax.set_title('KL Loss and β Schedule')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Semi-Supervised VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_semisupervised_vae(X, y, n_epochs=N_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                              mask_ratio=MASK_RATIO, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Train semi-supervised VAE with masked encoding, β-annealing, and α-annealing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Scaled input features\n",
    "    y : np.ndarray\n",
    "        Encoded labels\n",
    "    n_epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size\n",
    "    mask_ratio : float\n",
    "        Fraction of inputs to mask during training\n",
    "    device : torch.device\n",
    "        Device to train on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : SemiSupervisedVAE\n",
    "        Trained model\n",
    "    history : dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    set_seed(SEED)\n",
    "    \n",
    "    n_classes = len(np.unique(y))\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(torch.FloatTensor(X), torch.LongTensor(y))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SemiSupervisedVAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM,\n",
    "                              hidden_dims=HIDDEN_DIMS, n_classes=n_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'loss': [], 'recon_loss': [], 'kl_loss': [], 'class_loss': [], \n",
    "               'beta': [], 'alpha': [], 'accuracy': []}\n",
    "    \n",
    "    # Training loop\n",
    "    pbar = tqdm(range(n_epochs), desc='Training Semi-sup VAE')\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_recon = 0\n",
    "        epoch_kl = 0\n",
    "        epoch_class = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        beta = get_beta(epoch)\n",
    "        alpha = get_alpha(epoch)\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Apply masking\n",
    "            x_masked, mask = apply_mask(x_batch, mask_ratio)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar, logits = model(x_masked)\n",
    "            \n",
    "            # Loss\n",
    "            loss, recon_loss, kl_loss, class_loss = model.loss_function(\n",
    "                recon, x_batch, mu, logvar, logits, y_batch, beta, alpha\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            epoch_correct += (predicted == y_batch).sum().item()\n",
    "            epoch_total += y_batch.size(0)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_recon += recon_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "            epoch_class += class_loss.item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['recon_loss'].append(epoch_recon / n_batches)\n",
    "        history['kl_loss'].append(epoch_kl / n_batches)\n",
    "        history['class_loss'].append(epoch_class / n_batches)\n",
    "        history['beta'].append(beta)\n",
    "        history['alpha'].append(alpha)\n",
    "        history['accuracy'].append(epoch_correct / epoch_total)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{epoch_loss/n_batches:.1f}\",\n",
    "            'acc': f\"{epoch_correct/epoch_total:.3f}\",\n",
    "            'β': f\"{beta:.2e}\",\n",
    "            'α': f\"{alpha:.3f}\"\n",
    "        })\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train semi-supervised VAE\n",
    "print(f\"Training semi-supervised VAE on {len(X_scaled):,} samples...\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Classes: {n_classes}\")\n",
    "print(f\"  Epochs: {N_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Mask ratio: {MASK_RATIO}\")\n",
    "print(f\"  β-annealing: {BETA_START} → {BETA_END} over {BETA_ANNEAL_EPOCHS} epochs\")\n",
    "print(f\"  α-annealing: {ALPHA_START} → {ALPHA_END} over {ALPHA_ANNEAL_EPOCHS} epochs\")\n",
    "print()\n",
    "\n",
    "model_semisup, history_semisup = train_semisupervised_vae(X_scaled, y)\n",
    "\n",
    "# Save model\n",
    "torch.save(model_semisup.state_dict(), MODEL_DIR / 'model_semisup.pt')\n",
    "print(f\"\\nModel saved to {MODEL_DIR / 'model_semisup.pt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history_semisup['loss'], 'b-', linewidth=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Loss')\n",
    "ax.set_title('Semi-supervised VAE: Total Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history_semisup['recon_loss'], 'g-', linewidth=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Reconstruction Loss')\n",
    "ax.set_title('Reconstruction Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[0, 2]\n",
    "ax.plot(history_semisup['kl_loss'], 'r-', linewidth=1.5)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(history_semisup['beta'], 'b--', linewidth=1, alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('KL Loss', color='r')\n",
    "ax2.set_ylabel('β', color='b')\n",
    "ax.set_title('KL Loss and β Schedule')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history_semisup['class_loss'], 'purple', linewidth=1.5)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(history_semisup['alpha'], 'orange', linestyle='--', linewidth=1, alpha=0.7)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Classification Loss', color='purple')\n",
    "ax2.set_ylabel('α', color='orange')\n",
    "ax.set_title('Classification Loss and α Schedule')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history_semisup['accuracy'], 'teal', linewidth=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Training Accuracy')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare both models\n",
    "ax = axes[1, 2]\n",
    "ax.plot(history_unsup['recon_loss'], 'b-', linewidth=1.5, label='Unsupervised')\n",
    "ax.plot(history_semisup['recon_loss'], 'r-', linewidth=1.5, label='Semi-supervised')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Reconstruction Loss')\n",
    "ax.set_title('Reconstruction Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, X, device=DEVICE):\n",
    "    \"\"\"Extract embeddings from model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        embeddings = model.get_embeddings(X_tensor).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def get_reconstructions(model, X, device=DEVICE):\n",
    "    \"\"\"Get reconstructions using deterministic encoding (mu).\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        mu, _ = model.encode(X_tensor)\n",
    "        recon = model.decode(mu)\n",
    "    return recon.cpu().numpy()\n",
    "\n",
    "def get_predictions(model, X, device=DEVICE):\n",
    "    \"\"\"Get class predictions from semi-supervised model.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        mu, _ = model.encode(X_tensor)\n",
    "        logits = model.classify(mu)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "    return preds.cpu().numpy(), probs.cpu().numpy()\n",
    "\n",
    "def compute_ari(embeddings, y, k_values=[10, 12, 15, 20]):\n",
    "    \"\"\"Compute Adjusted Rand Index for k-means clustering.\"\"\"\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        ari = adjusted_rand_score(y, clusters)\n",
    "        results[k] = ari\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings and reconstructions\n",
    "print(\"Extracting embeddings and reconstructions...\")\n",
    "\n",
    "emb_unsup = get_embeddings(model_unsup, X_scaled)\n",
    "emb_semisup = get_embeddings(model_semisup, X_scaled)\n",
    "\n",
    "recon_unsup = get_reconstructions(model_unsup, X_scaled)\n",
    "recon_semisup = get_reconstructions(model_semisup, X_scaled)\n",
    "\n",
    "print(f\"  Unsupervised embeddings: {emb_unsup.shape}\")\n",
    "print(f\"  Semi-supervised embeddings: {emb_semisup.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.savez(MODEL_DIR / 'embeddings.npz',\n",
    "         unsup=emb_unsup, semisup=emb_semisup, y=y)\n",
    "print(f\"\\nEmbeddings saved to {MODEL_DIR / 'embeddings.npz'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction quality (R²)\n",
    "print(\"\\nReconstruction Quality (R²):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "r2_results = {'Feature': [], 'Unsupervised': [], 'Semi-supervised': []}\n",
    "\n",
    "for i, name in enumerate(FEATURE_NAMES_SHORT):\n",
    "    r2_unsup = r2_score(X_scaled[:, i], recon_unsup[:, i])\n",
    "    r2_semisup = r2_score(X_scaled[:, i], recon_semisup[:, i])\n",
    "    \n",
    "    r2_results['Feature'].append(name)\n",
    "    r2_results['Unsupervised'].append(r2_unsup)\n",
    "    r2_results['Semi-supervised'].append(r2_semisup)\n",
    "    \n",
    "    print(f\"  {name:15s}: Unsup R² = {r2_unsup:.4f}, Semi-sup R² = {r2_semisup:.4f}\")\n",
    "\n",
    "# Overall\n",
    "r2_overall_unsup = r2_score(X_scaled.flatten(), recon_unsup.flatten())\n",
    "r2_overall_semisup = r2_score(X_scaled.flatten(), recon_semisup.flatten())\n",
    "print(f\"\\n  {'Overall':15s}: Unsup R² = {r2_overall_unsup:.4f}, Semi-sup R² = {r2_overall_semisup:.4f}\")\n",
    "\n",
    "r2_df = pd.DataFrame(r2_results)\n",
    "r2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering metrics (ARI)\n",
    "print(\"\\nClustering Quality (ARI):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "ari_unsup = compute_ari(emb_unsup, y)\n",
    "ari_semisup = compute_ari(emb_semisup, y)\n",
    "\n",
    "print(\"\\nUnsupervised VAE:\")\n",
    "for k, ari in ari_unsup.items():\n",
    "    print(f\"  k={k}: ARI = {ari:.4f}\")\n",
    "\n",
    "print(\"\\nSemi-supervised VAE:\")\n",
    "for k, ari in ari_semisup.items():\n",
    "    print(f\"  k={k}: ARI = {ari:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classification on unsupervised embeddings\n",
    "print(\"\\nSVM Classification on Unsupervised Embeddings:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use subset for faster SVM training, filtering for classes with >=2 samples\n",
    "n_svm_samples = min(50000, len(y))\n",
    "svm_idx = np.random.choice(len(y), n_svm_samples, replace=False)\n",
    "\n",
    "# Filter to keep only classes with >=2 samples (required for stratified split)\n",
    "class_counts_svm = pd.Series(y[svm_idx]).value_counts()\n",
    "valid_classes = class_counts_svm[class_counts_svm >= 2].index.values\n",
    "valid_mask = np.isin(y[svm_idx], valid_classes)\n",
    "svm_idx_filtered = svm_idx[valid_mask]\n",
    "\n",
    "print(f\"Filtered from {len(svm_idx):,} to {len(svm_idx_filtered):,} samples\")\n",
    "print(f\"(keeping classes with >=2 samples for stratified split)\")\n",
    "\n",
    "# Train/test split\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
    "    emb_unsup[svm_idx_filtered], y[svm_idx_filtered], test_size=0.2, \n",
    "    random_state=SEED, stratify=y[svm_idx_filtered]\n",
    ")\n",
    "\n",
    "print(f\"Training SVM on {len(X_train_svm):,} samples...\")\n",
    "svm = SVC(kernel='rbf', C=1.0, random_state=SEED)\n",
    "svm.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_svm)\n",
    "acc_svm = accuracy_score(y_test_svm, y_pred_svm)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test_svm, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"\\nSVM Results (on unsupervised embeddings):\")\n",
    "print(f\"  Accuracy:  {acc_svm:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1:        {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-supervised classification head evaluation\n",
    "print(\"\\nSemi-supervised Classification Head Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "y_pred_semisup, y_prob_semisup = get_predictions(model_semisup, X_scaled)\n",
    "\n",
    "acc_semisup = accuracy_score(y, y_pred_semisup)\n",
    "precision_ss, recall_ss, f1_ss, _ = precision_recall_fscore_support(y, y_pred_semisup, average='weighted')\n",
    "\n",
    "print(f\"\\nClassification Head Results:\")\n",
    "print(f\"  Accuracy:  {acc_semisup:.4f}\")\n",
    "print(f\"  Precision: {precision_ss:.4f}\")\n",
    "print(f\"  Recall:    {recall_ss:.4f}\")\n",
    "print(f\"  F1:        {f1_ss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['R² (overall)', 'Accuracy', 'Precision', 'Recall', 'F1', f'ARI (k={list(ari_unsup.keys())[0]})'],\n",
    "    'Unsupervised (SVM)': [r2_overall_unsup, acc_svm, precision, recall, f1, list(ari_unsup.values())[0]],\n",
    "    'Semi-supervised': [r2_overall_semisup, acc_semisup, precision_ss, recall_ss, f1_ss, list(ari_semisup.values())[0]]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluation Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1 Reconstruction Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform to original scale for visualization\n",
    "X_orig_raw = scaler.inverse_transform(X_scaled)\n",
    "X_unsup_raw = scaler.inverse_transform(recon_unsup)\n",
    "X_semisup_raw = scaler.inverse_transform(recon_semisup)\n",
    "\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 8))\n",
    "\n",
    "for i, name in enumerate(FEATURE_NAMES_SHORT):\n",
    "    # Unsupervised (top row)\n",
    "    ax = axes[0, i]\n",
    "    r2 = r2_score(X_orig_raw[:, i], X_unsup_raw[:, i])\n",
    "    ax.scatter(X_orig_raw[:, i], X_unsup_raw[:, i], alpha=0.05, s=0.5, rasterized=True)\n",
    "    \n",
    "    # Reference line\n",
    "    lims = [min(X_orig_raw[:, i].min(), X_unsup_raw[:, i].min()),\n",
    "            max(X_orig_raw[:, i].max(), X_unsup_raw[:, i].max())]\n",
    "    ax.plot(lims, lims, 'r--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel(f'True {name}', fontsize=9)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Unsupervised\\nPredicted', fontsize=10)\n",
    "    ax.set_title(f'R²={r2:.3f}', fontsize=10)\n",
    "    \n",
    "    # Semi-supervised (bottom row)\n",
    "    ax = axes[1, i]\n",
    "    r2 = r2_score(X_orig_raw[:, i], X_semisup_raw[:, i])\n",
    "    ax.scatter(X_orig_raw[:, i], X_semisup_raw[:, i], alpha=0.05, s=0.5, rasterized=True, c='orange')\n",
    "    \n",
    "    lims = [min(X_orig_raw[:, i].min(), X_semisup_raw[:, i].min()),\n",
    "            max(X_orig_raw[:, i].max(), X_semisup_raw[:, i].max())]\n",
    "    ax.plot(lims, lims, 'r--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel(f'True {name}', fontsize=9)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Semi-supervised\\nPredicted', fontsize=10)\n",
    "    ax.set_title(f'R²={r2:.3f}', fontsize=10)\n",
    "\n",
    "plt.suptitle('Reconstruction Quality: True vs Predicted Values', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_reconstruction_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_reconstruction_scatter.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 ROC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 classes for ROC curve visualization\n",
    "class_counts = np.bincount(y)\n",
    "top10_classes = np.argsort(class_counts)[-10:]\n",
    "\n",
    "# Subsample for computational efficiency\n",
    "n_roc_samples = min(20000, len(y))\n",
    "roc_idx = np.random.choice(len(y), n_roc_samples, replace=False)\n",
    "\n",
    "# Filter to top 10 classes\n",
    "mask = np.isin(y[roc_idx], top10_classes)\n",
    "y_roc = y[roc_idx][mask]\n",
    "emb_unsup_roc = emb_unsup[roc_idx][mask]\n",
    "emb_semisup_roc = emb_semisup[roc_idx][mask]\n",
    "\n",
    "# Binarize labels\n",
    "y_bin = label_binarize(y_roc, classes=top10_classes)\n",
    "\n",
    "print(f\"ROC analysis on {len(y_roc):,} samples from top 10 classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# Unsupervised (SVM)\n",
    "print(\"Training SVM for unsupervised ROC...\")\n",
    "svm_roc = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=SEED))\n",
    "svm_roc.fit(emb_unsup_roc, y_bin)\n",
    "y_score_unsup = svm_roc.predict_proba(emb_unsup_roc)\n",
    "\n",
    "ax = axes[0]\n",
    "mean_auc_unsup = 0\n",
    "for i, cls in enumerate(top10_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score_unsup[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    mean_auc_unsup += roc_auc\n",
    "    name = label_encoder.inverse_transform([cls])[0][:15]\n",
    "    ax.plot(fpr, tpr, lw=2, alpha=0.8, color=colors[i], label=f'{name} ({roc_auc:.2f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax.set_title(f'Unsupervised VAE + SVM (mean AUC={mean_auc_unsup/10:.2f})', fontsize=12)\n",
    "ax.legend(loc='lower right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Semi-supervised (classification head)\n",
    "print(\"Computing semi-supervised ROC...\")\n",
    "_, y_prob_roc = get_predictions(model_semisup, X_scaled[roc_idx][mask])\n",
    "y_score_semisup = y_prob_roc[:, top10_classes]\n",
    "\n",
    "ax = axes[1]\n",
    "mean_auc_semisup = 0\n",
    "for i, cls in enumerate(top10_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_bin[:, i], y_score_semisup[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    mean_auc_semisup += roc_auc\n",
    "    name = label_encoder.inverse_transform([cls])[0][:15]\n",
    "    ax.plot(fpr, tpr, lw=2, alpha=0.8, color=colors[i], label=f'{name} ({roc_auc:.2f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "ax.set_title(f'Semi-supervised VAE (mean AUC={mean_auc_semisup/10:.2f})', fontsize=12)\n",
    "ax.legend(loc='lower right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ROC Curves: Top 10 Lithology Classes', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_roc_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_roc_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 R² Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "r2_unsup_list = r2_df['Unsupervised'].values\n",
    "r2_semisup_list = r2_df['Semi-supervised'].values\n",
    "features = r2_df['Feature'].values\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "for i, (r2_u, r2_s, name, color) in enumerate(zip(r2_unsup_list, r2_semisup_list, features, colors)):\n",
    "    ax.scatter(r2_u, r2_s, s=200, c=color, label=name, edgecolors='black', linewidth=1.5, zorder=5)\n",
    "\n",
    "# Diagonal line\n",
    "lims = [min(min(r2_unsup_list), min(r2_semisup_list)) - 0.02,\n",
    "        max(max(r2_unsup_list), max(r2_semisup_list)) + 0.02]\n",
    "ax.plot(lims, lims, 'k--', lw=2, alpha=0.5, label='Equal performance')\n",
    "\n",
    "ax.set_xlabel('R² Unsupervised VAE', fontsize=12)\n",
    "ax.set_ylabel('R² Semi-supervised VAE', fontsize=12)\n",
    "ax.set_title('Reconstruction Quality Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_r2_unsup_vs_semi.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_r2_unsup_vs_semi.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4 UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP embedding visualization\n",
    "print(\"Computing UMAP embeddings (this may take a few minutes)...\")\n",
    "\n",
    "# Subsample for UMAP\n",
    "n_umap_samples = min(50000, len(y))\n",
    "umap_idx = np.random.choice(len(y), n_umap_samples, replace=False)\n",
    "\n",
    "# Get top 15 classes for coloring\n",
    "top15_classes = set(np.argsort(class_counts)[-15:])\n",
    "\n",
    "print(f\"Computing UMAP for {n_umap_samples:,} samples...\")\n",
    "\n",
    "# UMAP for unsupervised\n",
    "print(\"  Unsupervised embeddings...\")\n",
    "reducer_unsup = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=SEED, n_jobs=-1)\n",
    "umap_unsup = reducer_unsup.fit_transform(emb_unsup[umap_idx])\n",
    "\n",
    "# UMAP for semi-supervised\n",
    "print(\"  Semi-supervised embeddings...\")\n",
    "reducer_semisup = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=SEED, n_jobs=-1)\n",
    "umap_semisup = reducer_semisup.fit_transform(emb_semisup[umap_idx])\n",
    "\n",
    "print(\"  Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "y_sub = y[umap_idx]\n",
    "\n",
    "# Create color array\n",
    "cmap = plt.cm.tab20(np.linspace(0, 1, 15))\n",
    "colors = np.array(['#CCCCCC'] * len(y_sub))\n",
    "for i, cls in enumerate(sorted(top15_classes)):\n",
    "    colors[y_sub == cls] = matplotlib.colors.to_hex(cmap[i])\n",
    "\n",
    "# Unsupervised\n",
    "ax = axes[0]\n",
    "ax.scatter(umap_unsup[:, 0], umap_unsup[:, 1], c=colors, s=1, alpha=0.5, rasterized=True)\n",
    "ax.set_xlabel('UMAP 1', fontsize=11)\n",
    "ax.set_ylabel('UMAP 2', fontsize=11)\n",
    "ax.set_title('Unsupervised VAE Embeddings', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Semi-supervised\n",
    "ax = axes[1]\n",
    "ax.scatter(umap_semisup[:, 0], umap_semisup[:, 1], c=colors, s=1, alpha=0.5, rasterized=True)\n",
    "ax.set_xlabel('UMAP 1', fontsize=11)\n",
    "ax.set_ylabel('UMAP 2', fontsize=11)\n",
    "ax.set_title('Semi-supervised VAE Embeddings', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "handles = []\n",
    "for i, cls in enumerate(sorted(top15_classes)):\n",
    "    name = label_encoder.inverse_transform([cls])[0][:20]\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                               markerfacecolor=matplotlib.colors.to_hex(cmap[i]),\n",
    "                               markersize=8, label=name))\n",
    "\n",
    "fig.legend(handles=handles, loc='center right', bbox_to_anchor=(1.12, 0.5), fontsize=9)\n",
    "\n",
    "plt.suptitle('UMAP Visualization of VAE Latent Space (colored by lithology)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_umap_lithology.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_umap_lithology.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Network Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vae_architecture(ax, model_type='unsupervised', n_classes=None):\n",
    "    \"\"\"\n",
    "    Draw VAE architecture diagram.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib axis\n",
    "    model_type : str\n",
    "        'unsupervised' or 'semi-supervised'\n",
    "    n_classes : int\n",
    "        Number of classes (for semi-supervised)\n",
    "    \"\"\"\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 6)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Colors\n",
    "    input_color = '#90EE90'   # Light green\n",
    "    hidden_color = '#87CEEB'  # Sky blue\n",
    "    latent_color = '#FFD700'  # Gold\n",
    "    output_color = '#FFA07A'  # Light salmon\n",
    "    class_color = '#DDA0DD'   # Plum\n",
    "    \n",
    "    # Layer positions (x)\n",
    "    x_input = 1\n",
    "    x_enc1 = 2.5\n",
    "    x_enc2 = 4\n",
    "    x_latent = 5.5\n",
    "    x_dec1 = 7\n",
    "    x_dec2 = 8.5\n",
    "    x_output = 10\n",
    "    \n",
    "    # Main path y-position\n",
    "    y_main = 3\n",
    "    \n",
    "    # Draw boxes\n",
    "    def draw_box(x, y, w, h, color, text, fontsize=9):\n",
    "        rect = plt.Rectangle((x - w/2, y - h/2), w, h, \n",
    "                             facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, text, ha='center', va='center', fontsize=fontsize, fontweight='bold')\n",
    "    \n",
    "    # Draw arrow\n",
    "    def draw_arrow(x1, y1, x2, y2, style='-'):\n",
    "        ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=1.5, ls=style))\n",
    "    \n",
    "    # Input\n",
    "    draw_box(x_input, y_main, 0.8, 1.2, input_color, 'Input\\n(6)')\n",
    "    \n",
    "    # Encoder\n",
    "    draw_box(x_enc1, y_main, 0.8, 1.2, hidden_color, 'FC\\n64\\nBN')\n",
    "    draw_box(x_enc2, y_main, 0.8, 1.2, hidden_color, 'FC\\n32\\nBN')\n",
    "    \n",
    "    # Latent (mu and logvar)\n",
    "    draw_box(x_latent, y_main + 0.7, 0.8, 0.8, latent_color, 'μ (10)')\n",
    "    draw_box(x_latent, y_main - 0.7, 0.8, 0.8, latent_color, 'σ² (10)')\n",
    "    \n",
    "    # Decoder\n",
    "    draw_box(x_dec1, y_main, 0.8, 1.2, hidden_color, 'FC\\n32\\nBN')\n",
    "    draw_box(x_dec2, y_main, 0.8, 1.2, hidden_color, 'FC\\n64\\nBN')\n",
    "    \n",
    "    # Output\n",
    "    draw_box(x_output - 0.5, y_main, 0.8, 1.2, output_color, 'Output\\n(6)')\n",
    "    \n",
    "    # Arrows - encoder\n",
    "    draw_arrow(x_input + 0.4, y_main, x_enc1 - 0.4, y_main)\n",
    "    draw_arrow(x_enc1 + 0.4, y_main, x_enc2 - 0.4, y_main)\n",
    "    draw_arrow(x_enc2 + 0.4, y_main + 0.2, x_latent - 0.4, y_main + 0.7)\n",
    "    draw_arrow(x_enc2 + 0.4, y_main - 0.2, x_latent - 0.4, y_main - 0.7)\n",
    "    \n",
    "    # Sampling (z)\n",
    "    ax.text(x_latent + 0.6, y_main, 'z', fontsize=12, fontweight='bold', ha='center', va='center')\n",
    "    draw_arrow(x_latent + 0.4, y_main + 0.5, x_latent + 0.6, y_main + 0.1)\n",
    "    draw_arrow(x_latent + 0.4, y_main - 0.5, x_latent + 0.6, y_main - 0.1)\n",
    "    \n",
    "    # Arrows - decoder\n",
    "    draw_arrow(x_latent + 0.8, y_main, x_dec1 - 0.4, y_main)\n",
    "    draw_arrow(x_dec1 + 0.4, y_main, x_dec2 - 0.4, y_main)\n",
    "    draw_arrow(x_dec2 + 0.4, y_main, x_output - 0.5 - 0.4, y_main)\n",
    "    \n",
    "    # Classification head for semi-supervised\n",
    "    if model_type == 'semi-supervised' and n_classes:\n",
    "        y_class = 1.2\n",
    "        draw_box(x_dec1, y_class, 0.8, 0.8, class_color, 'FC\\n64')\n",
    "        draw_box(x_dec2, y_class, 0.8, 0.8, class_color, f'FC\\n{n_classes}')\n",
    "        \n",
    "        # Arrows from z to classifier\n",
    "        draw_arrow(x_latent + 0.6, y_main - 0.3, x_dec1 - 0.4, y_class)\n",
    "        draw_arrow(x_dec1 + 0.4, y_class, x_dec2 - 0.4, y_class)\n",
    "        \n",
    "        # Label\n",
    "        ax.text(x_dec2 + 0.8, y_class, 'Classes', fontsize=10, ha='left', va='center')\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(x_enc1 + 0.75, y_main + 1.5, 'Encoder', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.text(x_dec1 + 0.75, y_main + 1.5, 'Decoder', fontsize=11, ha='center', fontweight='bold')\n",
    "    ax.text(x_latent, y_main + 1.8, 'Latent Space', fontsize=11, ha='center', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw both architectures\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Unsupervised\n",
    "ax = axes[0]\n",
    "draw_vae_architecture(ax, model_type='unsupervised')\n",
    "ax.set_title('Unsupervised VAE Architecture', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Semi-supervised\n",
    "ax = axes[1]\n",
    "draw_vae_architecture(ax, model_type='semi-supervised', n_classes=n_classes)\n",
    "ax.set_title('Semi-Supervised VAE Architecture', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'fig_network_diagrams.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIGURES_DIR / 'fig_network_diagrams.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"BHAI VAE PAPER NOTEBOOK - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTimestamp: {datetime.now().isoformat()}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Random seed: {SEED}\")\n",
    "\n",
    "print(f\"\\n--- Dataset ---\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Total boreholes: {df['Borehole_ID'].nunique():,}\")\n",
    "print(f\"Expeditions: {len(expeditions)}\")\n",
    "print(f\"Lithology classes: {n_classes}\")\n",
    "\n",
    "print(f\"\\n--- Model Architecture ---\")\n",
    "print(f\"Input dim: {INPUT_DIM}\")\n",
    "print(f\"Latent dim: {LATENT_DIM}\")\n",
    "print(f\"Hidden dims: {HIDDEN_DIMS}\")\n",
    "\n",
    "print(f\"\\n--- Training ---\")\n",
    "print(f\"Epochs: {N_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Mask ratio: {MASK_RATIO}\")\n",
    "print(f\"β-annealing: {BETA_START} → {BETA_END} over {BETA_ANNEAL_EPOCHS} epochs\")\n",
    "print(f\"α-annealing: {ALPHA_START} → {ALPHA_END} over {ALPHA_ANNEAL_EPOCHS} epochs\")\n",
    "\n",
    "print(f\"\\n--- Results ---\")\n",
    "print(f\"Unsupervised VAE:\")\n",
    "print(f\"  R² (overall): {r2_overall_unsup:.4f}\")\n",
    "print(f\"  SVM Accuracy: {acc_svm:.4f}\")\n",
    "print(f\"\\nSemi-supervised VAE:\")\n",
    "print(f\"  R² (overall): {r2_overall_semisup:.4f}\")\n",
    "print(f\"  Classification Accuracy: {acc_semisup:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Generated Files ---\")\n",
    "print(f\"Models:\")\n",
    "for f in MODEL_DIR.glob('*.pt'):\n",
    "    print(f\"  {f.name}\")\n",
    "print(f\"\\nFigures:\")\n",
    "for f in sorted(FIGURES_DIR.glob('fig_*.png')):\n",
    "    print(f\"  {f.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
