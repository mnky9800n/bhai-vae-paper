# Models

**Last updated:** 2026-02-05

## Architecture Definitions

All model architectures are defined in `models/vae.py` — this is the **single source of truth**.

### DistributionAwareScaler

Custom preprocessing that applies distribution-specific transforms before StandardScaler:

```python
Column indices:
  [0] Bulk density      → no transform (already ~normal)
  [1] Mag susceptibility → signed_log: sign(x) * log1p(|x|)
  [2] NGR counts        → signed_log: sign(x) * log1p(|x|)
  [3] R                 → log1p(x)
  [4] G                 → log1p(x)
  [5] B                 → log1p(x)

Then: StandardScaler on all columns
```

### VAE (Unsupervised)

```
Input: 6 features (scaled)

Encoder:
  Linear(6 → 64) → ReLU → BatchNorm1d(64)
  Linear(64 → 32) → ReLU → BatchNorm1d(32)
  
Latent:
  fc_mu: Linear(32 → 10)
  fc_logvar: Linear(32 → 10)
  
Reparameterization:
  z = mu + std * eps, where std = exp(0.5 * logvar), eps ~ N(0,1)

Decoder:
  Linear(10 → 32) → ReLU → BatchNorm1d(32)
  Linear(32 → 64) → ReLU → BatchNorm1d(64)
  Linear(64 → 6)

Output: 6 features (scaled space)
```

**Parameters:** 6,426 total

### SemiSupervisedVAE

Same encoder/decoder as VAE, plus classification head:

```
Classifier (from latent z):
  Linear(10 → 64) → ReLU → Dropout(0.3) → Linear(64 → 139)

Output: 139 lithology class logits
```

**Parameters:** 16,165 total (6,426 base + 9,739 classifier)

---

## Saved Model Files

### models/unsup.pt

| Property | Value |
|----------|-------|
| Architecture | VAE |
| Input dim | 6 |
| Latent dim | 10 |
| Hidden dims | [64, 32] |
| Parameters | 6,426 |
| File size | 38,610 bytes |
| Created | 2026-02-01 21:01 |
| Trained by | `scripts/train_vae.py` |
| Training method | Standard (β=1.0) |

### models/semisup.pt

| Property | Value |
|----------|-------|
| Architecture | SemiSupervisedVAE |
| Input dim | 6 |
| Latent dim | 10 |
| Hidden dims | [64, 32] |
| N classes | 139 |
| Parameters | 16,165 |
| File size | 79,046 bytes |
| Created | 2026-02-01 21:01 |
| Trained by | `scripts/train_vae.py` |
| Training method | Standard (β=1.0, α=0.1) |

### models/model_unsup_hybrid.pt

| Property | Value |
|----------|-------|
| Architecture | VAE |
| Input dim | 6 |
| Latent dim | 10 |
| Hidden dims | [64, 32] |
| Parameters | 6,426 |
| File size | 39,029 bytes |
| Created | 2026-02-05 17:55 |
| Trained by | `scripts/generate_all_figures.py` |
| Training method | Hybrid (masking, KL annealing) |

### models/model_semisup_hybrid.pt

| Property | Value |
|----------|-------|
| Architecture | SemiSupervisedVAE |
| Input dim | 6 |
| Latent dim | 10 |
| Hidden dims | [64, 32] |
| N classes | 139 |
| Parameters | 16,165 |
| File size | 79,469 bytes |
| Created | 2026-02-05 18:01 |
| Trained by | `scripts/generate_all_figures.py` |
| Training method | Hybrid (masking, KL annealing, α=0.5) |

### models/embeddings_hybrid.npz

NumPy archive containing pre-computed embeddings from hybrid models:

```python
import numpy as np
data = np.load('models/embeddings_hybrid.npz')
# data['unsup']   - shape (238506, 10) - unsupervised embeddings
# data['semisup'] - shape (238506, 10) - semi-supervised embeddings
# data['y']       - shape (238506,)    - lithology labels (encoded)
```

| Property | Value |
|----------|-------|
| File size | 20,989,272 bytes (~20MB) |
| Created | 2026-02-05 18:01 |
| Generated by | `scripts/generate_all_figures.py` |

---

## Loss Functions

### VAE Loss

```python
def loss_function(recon, x, mu, logvar, beta=1.0):
    recon_loss = MSE(recon, x, reduction='sum')
    kl_loss = -0.5 * sum(1 + logvar - mu² - exp(logvar))
    return recon_loss + beta * kl_loss
```

### SemiSupervisedVAE Loss

```python
def loss_function(recon, x, mu, logvar, logits, labels, beta=1.0, alpha=0.1):
    recon_loss = MSE(recon, x, reduction='sum')
    kl_loss = -0.5 * sum(1 + logvar - mu² - exp(logvar))
    class_loss = CrossEntropy(logits, labels, reduction='sum')
    return recon_loss + beta * kl_loss + alpha * class_loss
```

### Hybrid Loss (generate_all_figures.py)

```python
# Additional masking loss
mask = random(x) < 0.1  # 10% mask ratio
x_masked = x.clone()
x_masked[mask] = 0

recon_loss = MSE(recon, x, reduction='mean')
masked_loss = MSE(recon[mask], x[mask], reduction='mean')
kl_loss = -0.5 * mean(1 + logvar - mu² - exp(logvar))

loss = recon_loss + 0.5 * masked_loss + beta * kl_loss
# beta anneals from 1e-6 to 0.5 over 30 epochs

# For semi-supervised:
loss = loss + 0.5 * CrossEntropy(logits, labels)
```

---

## Loading Models

```python
import torch
from models.vae import VAE, SemiSupervisedVAE

# Unsupervised
model = VAE(input_dim=6, latent_dim=10, hidden_dims=[64, 32])
model.load_state_dict(torch.load('models/unsup.pt', map_location='cpu'))
model.eval()

# Semi-supervised
model = SemiSupervisedVAE(input_dim=6, latent_dim=10, hidden_dims=[64, 32], n_classes=139)
model.load_state_dict(torch.load('models/semisup.pt', map_location='cpu'))
model.eval()
```

## Getting Embeddings

```python
import torch
import numpy as np

# Assuming X_scaled is your preprocessed data (N, 6)
with torch.no_grad():
    X_tensor = torch.FloatTensor(X_scaled)
    embeddings = model.get_embeddings(X_tensor).numpy()  # (N, 10)
```
