# Issues and Inconsistencies

**Last updated:** 2026-02-05

This document catalogs inconsistencies, unknowns, and potential problems in the codebase.

---

## üî¥ Critical Issues

### 1. Two Training Methods Produce Incompatible Models

**Problem:** The codebase has two different training procedures:

| Script | Output | Training Method |
|--------|--------|-----------------|
| `train_vae.py` | `unsup.pt`, `semisup.pt` | Standard (Œ≤=1.0 fixed) |
| `generate_all_figures.py` | `model_*_hybrid.pt` | Hybrid (masking, KL annealing) |

**Impact:** 
- Figures generated by different scripts use different models
- Results are not directly comparable
- README documents standard training but figures use hybrid

**Files affected:**
- `fig_r2_unsup_vs_semi.png` uses original models
- All other figures use hybrid models

**Recommendation:** Standardize on one training approach and retrain all models.

---

### 2. Inconsistent Œ± Values

**Problem:** Classification weight differs between scripts:

| Location | Œ± value |
|----------|---------|
| `train_vae.py` (default) | 0.1 |
| `generate_all_figures.py` | 0.5 |
| `README.md` | 0.1 |

**Impact:** Documentation doesn't match actual model training.

**Recommendation:** Pick one value and update all locations.

---

### 3. Figure Uses Wrong Models

**Problem:** `fig_r2_unsup_vs_semi.png` is generated by `run_bootstrap_1337.py` which loads:
- `models/unsup.pt`
- `models/semisup.pt`

But other figures use:
- `models/model_unsup_hybrid.pt`
- `models/model_semisup_hybrid.pt`

**Impact:** The R¬≤ comparison figure uses different models than the reconstruction/ROC figures.

**Recommendation:** Either regenerate R¬≤ figure with hybrid models, or regenerate all other figures with original models.

---

## üü° Moderate Issues

### 4. Orphaned Figures (No Script)

**Problem:** Four figures have no Python script that generates them:
- `fig_lily_dataset.png`
- `fig_lily_expedition_map.png`
- `fig_lily_lithology_counts.png`
- `fig_lily_variables_dist.png`

**Likely source:** `notebooks/paper_figures.ipynb`

**Impact:** Cannot regenerate these figures programmatically.

**Recommendation:** Extract figure generation code from notebook into scripts.

---

### 5. Hardcoded External Paths

**Problem:** Multiple scripts contain hardcoded absolute paths:

```python
# run_bootstrap_1337.py, run_bootstrap_full.py
LILY_DIR = Path('/home/mnky9800n/clawd/data/lily-datasets')

# fig_r2_scatter_with_ci.py
results_path = Path("/home/mnky9800n/clawd/bhai-analysis/zeroshot_scatter_results_full.csv")
```

**Impact:** Scripts won't work on other machines or after directory restructuring.

**Recommendation:** Use environment variables or config file:
```python
LILY_DIR = Path(os.environ.get('LILY_DIR', 'data/lily-datasets'))
```

---

### 6. Duplicate Model Training Code

**Problem:** `generate_all_figures.py` contains its own `train_model()` function instead of using `train_vae.py`.

**Impact:** 
- Bug fixes must be applied in two places
- Easy to introduce inconsistencies

**Recommendation:** Refactor `train_model()` into a shared module or call `train_vae.py` as subprocess.

---

### 7. remake_reconstruction_scatter.py Overwrites with Different Results

**Problem:** Running `remake_reconstruction_scatter.py` will overwrite `fig_reconstruction_scatter.png` using original models, but `generate_all_figures.py` creates it with hybrid models.

**Impact:** Unintended changes to figures.

**Recommendation:** Either delete script or rename output to avoid collision.

---

## üü¢ Minor Issues

### 8. Missing .gitignore Entries

**Problem:** Generated files that shouldn't be tracked:
- `models/embeddings_hybrid.npz` (20MB)
- `data/embeddings.csv` (80MB)
- Various `*.log` files

**Recommendation:** Update `.gitignore`:
```
models/*.npz
data/embeddings.csv
*.log
catboost_info/
```

---

### 9. Stale Documentation

**Problem:** README.md describes workflow that doesn't match current scripts:
- Mentions `notebooks/paper_figures.ipynb` as primary figure source
- Doesn't mention `generate_all_figures.py` as main script
- Documents Œ±=0.1 but hybrid uses Œ±=0.5

**Recommendation:** Update README to reflect current workflow.

---

### 10. Loss Reduction Inconsistency

**Problem:** 
- `train_vae.py` uses `reduction='sum'` 
- `generate_all_figures.py` uses `reduction='mean'`

**Impact:** Loss values not directly comparable between runs.

**Recommendation:** Standardize on one approach (sum is more common for VAEs).

---

## ‚ùì Unknowns

### 11. Data Provenance

**Questions:**
- How was `vae_training_data_v2_20cm.csv` generated?
- What is "v2" vs v1?
- Why 20cm bins specifically?

**No documentation found.**

---

### 12. Model Version Names

**Questions:**
- Why are models called "v2.6.7" and "v2.14"?
- What were previous versions?
- Is there a changelog?

**Context:** Column names in results use `r2_v267` and `r2_v214`.

---

### 13. 139 Lithology Classes

**Questions:**
- What are all 139 classes?
- Are they hierarchical?
- Why this number?

**The `Principal` column values are not documented.**

---

## Checklist for Cleanup

- [ ] Decide on single training approach (standard or hybrid)
- [ ] Retrain all models with consistent hyperparameters
- [ ] Regenerate all figures with same models
- [ ] Extract notebook figures into scripts
- [ ] Remove hardcoded paths
- [ ] Update README and documentation
- [ ] Add proper .gitignore entries
- [ ] Document data provenance
- [ ] Add lithology class mapping

---

## Related: bhai-analysis Repo

The sibling repo `/home/mnky9800n/clawd/bhai-analysis` contains:
- Earlier experimental scripts
- Duplicate data files (identical MD5)
- Some results files referenced by bhai-vae-paper

**Relationship unclear:** Is bhai-analysis deprecated? Archived? Still active?

**Files shared between repos:**
- `vae_training_data_v2_20cm.csv` (identical)
- `zeroshot_scatter_results_full.csv` (referenced from bhai-analysis)
- Model architecture (likely originated in bhai-analysis)

**Recommendation:** Either:
1. Merge all needed files into bhai-vae-paper and archive bhai-analysis
2. Set up bhai-analysis as a dependency
3. Document the relationship clearly
